# 知识库测试参数对比说明

## ✅ 统一的测试参数

所有知识库适配器在测试时使用完全相同的参数，确保公平对比：

### 1. 查询参数
- **常规查询**: `top_k=5` - 返回5个最相关的文档
- **质量评估**: `top_k=10` - 返回10个文档用于计算精确度和MRR

### 2. 测试数据规模
| 规模 | 文档数 | 查询数 | 说明 |
|------|--------|--------|------|
| existing | 100 | 5 | 文档已预先入库，仅测试查询 |
| tiny | 10 | 5 | 快速测试 |
| small | 100 | 50 | 标准测试 |
| medium | 1000 | 200 | 压力测试 |
| large | 10000 | 500 | 大规模测试 |

### 3. 测试查询集合
- 所有适配器使用完全相同的查询语句
- 查询通过 `TestDataGenerator` 统一生成
- 质量评估时使用 `test-data` 目录下的真实文档

### 4. 测试流程
1. 初始化适配器
2. 上传文档（existing模式跳过）
3. 构建索引（existing模式跳过）
4. 执行查询测试（记录延迟）
5. 计算指标（延迟、吞吐、质量）

## 🔍 适配器内部实现差异

虽然所有适配器接收相同的 `top_k` 参数，但内部检索机制不同：

### AWS Bedrock KB
```python
retrieveRequest = {
    "numberOfResults": top_k  # 直接返回5个结果
}
```
- **检索方式**: 单次检索，直接返回top_k个结果
- **延迟特点**: 较低（~50-200ms）

### 火山引擎 VikingDB
```python
search_params = {
    "limit": top_k,  # 直接返回5个结果
    "dense_weight": 0.5
}
```
- **检索方式**: 混合检索（稠密+稀疏），直接返回top_k个结果
- **延迟特点**: 中等（~100-500ms）

### 阿里百炼（优化前）
```python
retrieve_request = {
    "dense_similarity_top_k": 100,   # 召回100个稠密向量结果
    "sparse_similarity_top_k": 100,  # 召回100个稀疏向量结果
    "enable_reranking": True,        # 启用重排序
    "rerank_top_n": 5               # 重排序后返回5个
}
```
- **检索方式**: 两阶段检索
  1. 召回：100 (稠密) + 100 (稀疏) = 200个候选
  2. 重排序：从200个候选中选出最佳的5个
- **延迟特点**: 非常高（~3000-9000ms）
- **问题**: 为了返回5个结果，需要处理200个候选文档

### 阿里百炼（优化后）
```python
retrieve_request = {
    "dense_similarity_top_k": 20,    # ↓ 召回20个稠密向量结果
    "sparse_similarity_top_k": 20,   # ↓ 召回20个稀疏向量结果
    "enable_reranking": True,        # 保持启用
    "rerank_top_n": 10              # ↑ 返回10个（适配质量评估）
}
```
- **检索方式**: 优化后的两阶段检索
  1. 召回：20 (稠密) + 20 (稀疏) = 40个候选
  2. 重排序：从40个候选中选出最佳的10个
- **延迟预期**: 降低至~500-1000ms（优化80%）
- **优化原因**:
  - 减少80%的召回数量（200→40）
  - 保持重排序以确保质量
  - 增加最终返回数量以满足质量评估

## 📊 性能对比分析

### 延迟差异的主要原因

1. **召回阶段开销**
   - AWS/火山引擎: 单次检索，直接返回结果
   - 阿里百炼: 需要两次独立的相似度检索（稠密+稀疏）

2. **候选文档数量**
   - AWS/火山引擎: 处理5-10个文档
   - 阿里百炼（优化前）: 处理200个候选文档
   - 阿里百炼（优化后）: 处理40个候选文档

3. **重排序开销**
   - AWS/火山引擎: 不需要额外重排序
   - 阿里百炼: 需要对所有候选文档进行重排序（使用更复杂的排序模型）

### 为什么阿里百炼采用两阶段检索

两阶段检索（召回+重排序）是工业界常见的做法：

**优势：**
- 更高的检索质量（重排序模型更精确）
- 混合检索（稠密+稀疏）可以兼顾语义和关键词匹配
- 可以处理更复杂的查询意图

**劣势：**
- 延迟更高（需要处理更多候选文档）
- 计算成本更高

**优化建议：**
- 对于延迟敏感的应用：降低召回数量（如本次优化）
- 对于质量优先的应用：可以保持较高的召回数量

## 🎯 测试结论

所有适配器在测试时：
- ✅ 使用相同的查询语句
- ✅ 返回相同数量的结果（5或10个）
- ✅ 使用相同的数据规模
- ✅ 使用相同的评估指标

但由于内部实现机制不同，导致性能表现差异：
- **延迟**: 取决于检索架构（单次 vs 两阶段）和候选数量
- **质量**: 取决于检索算法和重排序模型
- **吞吐**: 取决于延迟和并发能力

## 📝 配置建议

如果需要调整阿里百炼的检索参数，可以在 `config/config.yaml` 中配置：

```yaml
aliyun:
  # 基础配置
  workspace_id: "your_workspace_id"
  index_id: "your_index_id"

  # 性能优化配置（可选）
  dense_similarity_top_k: 20      # 稠密向量召回数（默认20）
  sparse_similarity_top_k: 20     # 稀疏向量召回数（默认20）
  enable_reranking: true          # 是否启用重排序（默认true）
  rerank_top_n: 10                # 重排序后返回数（默认10）
  rerank_min_score: 0.01          # 最低相关度分数（默认0.01）
```

**推荐配置：**
- **性能优先**: `dense_similarity_top_k: 10, sparse_similarity_top_k: 10, enable_reranking: false`
- **平衡模式**: `dense_similarity_top_k: 20, sparse_similarity_top_k: 20, enable_reranking: true` (当前默认)
- **质量优先**: `dense_similarity_top_k: 50, sparse_similarity_top_k: 50, enable_reranking: true, rerank_top_n: 20`
